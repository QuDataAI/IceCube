{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM1w8SkIVztR"
      },
      "source": [
        "# IceCube: Creating a dataset\n",
        "\n",
        "This notebook creates a dataset on google drive to train the model.\n",
        "Initial training batches are also uploaded to google drive.\n",
        "\n",
        "Every 5 training batches were collected in a pack of 1m examples.<br>\n",
        "These examples were sorted by sequence length and combined into groups of the same length. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WYz0_5vV__-"
      },
      "source": [
        "## Load data from disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaZoehvvmAj5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "CREATE_DATASET      = True          # start dataset generation\n",
        "LOAD_FROM_DISK      = True          # download kaggle source batches from google drive\n",
        "\n",
        "DROP_AUX            = False         # throw out aux==True\n",
        "DOMS_AGG            = False         # aggregate by sensors (time of the first)\n",
        "T_MAX               = 512           # maximum number of pulses in an event\n",
        "IS_AGG              = True          # generate aggregated event features\n",
        "\n",
        "LINES_FILTER        = False         # set a filter by the number of strings in the event\n",
        "LINES_MIN           = 0             # minimum number of strings (if LINES_FILTER==True)\n",
        "LINES_MAX           = 100           # maximum number of strings (if LINES_FILTER==True)\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "FIRST_BATCH_ID   = 1                # number of the first batch for training and validation\n",
        "NUM_BATCHES      = 10               # total number of batches for training and validation\n",
        "BATCHES_IN_PACK  = 5                # number of batches per group for training and validation\n",
        "\n",
        "# folder for the resulting dataset:\n",
        "DATASET_FOLDER = \"/content/drive/MyDrive/IceCube/IceCube-Dataset/ALL_512\"\n",
        "\n",
        "#===============================================================================\n",
        "\n",
        "import os, gc, sys, time, datetime, math, random,  psutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from   pathlib   import Path        \n",
        "from   tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import pyarrow, pyarrow.parquet as pq     # read by chanks\n",
        "import torch\n",
        "\n",
        "from psutil import virtual_memory\n",
        "print(f'Your runtime has {(virtual_memory().total / 1024**3):.1f} gigabytes of available RAM\\n')\n",
        "#===============================================================================\n",
        "# Copying competition batches from google drive\n",
        "if LOAD_FROM_DISK:\n",
        "    !cp /content/drive/MyDrive/IceCube/IceCube-Dataset/train_meta_splitted.zip /content/\n",
        "    !unzip -q /content/train_meta_splitted.zip\n",
        "    !rm       /content/train_meta_splitted.zip\n",
        "\n",
        "    !cp /content/drive/MyDrive/IceCube/IceCube-Dataset/sensor_geometry.csv /content/\n",
        "    !cp /content/drive/MyDrive/IceCube/IceCube-Dataset/scattering_and_absorption.csv /content/\n",
        "            \n",
        "    for batch_id in tqdm(range(FIRST_BATCH_ID, FIRST_BATCH_ID + NUM_BATCHES)):\n",
        "        !cp    /content/drive/MyDrive/IceCube/IceCube-Dataset/train/batch_{batch_id}.parquet /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM_XS5s9VnIy"
      },
      "source": [
        "## Prepare Data Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt7JD7zxmjyZ"
      },
      "outputs": [],
      "source": [
        "#===============================================================================\n",
        "PATH      = Path(\"/content\")                  # path to dataset\n",
        "PATH_PHYS = Path(\"/content\")                  # path to dataset\n",
        "PATH_META = Path(\"/content/content/icecube-neutrinos-in-deep-ice/train_meta\")\n",
        "files_trn = [item for item in (PATH  / \"train\").glob('*')]  # all train files\n",
        "print(f\"{len(files_trn):3d} train files\")\n",
        "#===============================================================================\n",
        "\n",
        "def info(text, pref=\"\", end=\"\\n\"):\n",
        "    \"\"\" \n",
        "    Information about the progress of calculations (time and memory) \n",
        "    \"\"\"\n",
        "    gc.collect()\n",
        "    ram, t = psutil.virtual_memory().used / 1024**3,  time.time()    \n",
        "    print(f\"{pref}{(t-info.beg)/60:5.1f}m[{t-info.last:+5.1f}s] {ram:6.3f}Gb > {text}\",end=end)\n",
        "    info.last = time.time(); \n",
        "info.beg = info.last = time.time()\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def get_sensors():\n",
        "    \"\"\" \n",
        "    Get sensor positions \n",
        "    \"\"\"            \n",
        "    df = pd.read_csv(PATH / \"sensor_geometry.csv\")      \n",
        "    df['line_id'] = df.sensor_id // 60 + 1                 # string id\n",
        "    df['core']    = (df.line_id > 78).astype(np.float32)   # sensor from DeepCore\n",
        "    df.x = ( df.x * 1e-3 ).astype(np.float32)              # distances in kilometers\n",
        "    df.y = ( df.y * 1e-3 ).astype(np.float32)\n",
        "    df.z = ( df.z * 1e-3 ).astype(np.float32)    \n",
        "    \n",
        "    from scipy.interpolate import interp1d                 # add absorption\n",
        "    phys = pd.read_csv(PATH_PHYS / \"scattering_and_absorption.csv\")\n",
        "    phys.z = (phys.z * 1e-3).astype(np.float32)\n",
        "    phys.a = (phys.a * 1e-2).astype(np.float32)\n",
        "    interp = interp1d(phys.z, phys.a)\n",
        "    df['a'] = interp(df.z)\n",
        "\n",
        "    df['r'] = np.sqrt(df.x**2 + df.y**2)\n",
        "\n",
        "    return df[['sensor_id', 'line_id', 'core', 'x', 'y', 'z', 'a', 'r']]\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def get_target_angles(batch_id=1):\n",
        "    \"\"\" \n",
        "    Get target angles for batch with batch_id \n",
        "    \"\"\"    \n",
        "    assert batch_id > 0 and  batch_id < 661, \"Wrong batch_id\"        \n",
        "    df = pd.read_parquet(PATH_META / f\"batch_{batch_id}_meta.parquet\")\n",
        "    df.event_id= df.event_id.astype(np.int64)      \n",
        "    df.azimuth = df.azimuth.astype(np.float32)\n",
        "    df.zenith  = df.zenith.astype(np.float32)                        \n",
        "    return df[ ['event_id','azimuth','zenith'] ]\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_batch(df, verbose=True, drop_aux = DROP_AUX, doms_agg = DOMS_AGG):\n",
        "    \"\"\" \n",
        "    Preparing a loaded batch, shifting and normalizing times \n",
        "    \"\"\"    \n",
        "    df['event_id'] = df.index.astype(np.int64)\n",
        "    df = df.reset_index(drop=True)  # sensor_id, t, charge, aux, event_id    \n",
        "    df.rename(columns={\"time\": \"t\", \"auxiliary\": \"aux\", 'charge': 'q'}, inplace=True)\n",
        "    df.q = df.q.astype(np.float32)\n",
        "\n",
        "    if drop_aux:\n",
        "        df = df[ ~df.aux ]\n",
        "    \n",
        "    if doms_agg:\n",
        "        df = df.groupby(['event_id', 'sensor_id']).agg(\n",
        "            aux = ( 'aux', \"mean\"),\n",
        "            q   = ( 'q',   \"sum\"),\n",
        "            t   = ( 't',   \"min\"),            \n",
        "        )\n",
        "        df = df.reset_index()\n",
        "    \n",
        "    if verbose: info(f\"load_batch: loaded  {df.shape}\")\n",
        "        \n",
        "    times = df.groupby('event_id').agg( t_min = ('t', 'min') )\n",
        "    df = df.merge(times, left_on='event_id', right_index=True, how='left')\n",
        "    df.t = (( df.t - df.t_min ) * 0.299792458e-3 ).astype(np.float32)             \n",
        "    \n",
        "    if verbose: info(\"load_batch: shift_times\")    \n",
        "\n",
        "    return df[['event_id', 'sensor_id', 'aux', 'q', 't' ]]\n",
        "        \n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def cut_pulses(df, max_pulses = 128, verbose=True):\n",
        "    \"\"\" \n",
        "    Throw out the last and unreliable pulses in the event if there are more than max_pulses \n",
        "    \"\"\"\n",
        "    tot = len(df)\n",
        "    df = df.sort_values(['event_id','aux','t'])          # do you need aux???\n",
        "    df.reset_index(drop=True, inplace=True)    \n",
        "\n",
        "    df = df.groupby('event_id').head(max_pulses)         # cut pulses by event\n",
        "    df.reset_index(inplace=True)                         # sorted by time later!\n",
        "\n",
        "    if not DROP_AUX:\n",
        "        df = df.sort_values(['event_id','t'])        \n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    if verbose: info(f\"cut_pulses (max={max_pulses}): removed {100*(tot-len(df))/tot:.2f}%\")\n",
        "    return df\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def lines_filter(df, lines_min=1, lines_max=1):\n",
        "    \"\"\"\n",
        "    Filter by number of strings\n",
        "    \"\"\"\n",
        "    agg = df[df.aux == 0].copy().groupby('event_id').agg( lines0 = ( 'line_id',   'nunique') )\n",
        "    agg.reset_index(inplace=True)    \n",
        "    agg = agg[(agg.lines0 >= lines_min) & (agg.lines0 <= lines_max)]\n",
        "    df = df[df.event_id.isin(agg.event_id)]\n",
        "    df.reset_index(drop=True, inplace=True)  \n",
        "    return df, agg\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def angles2vector(df):\n",
        "    \"\"\" \n",
        "    Add unit vector components from (azimuth,zenith) to the DataFrame df \n",
        "    \"\"\"\n",
        "    df['nx'] = np.sin(df.zenith) * np.cos(df.azimuth)\n",
        "    df['ny'] = np.sin(df.zenith) * np.sin(df.azimuth)\n",
        "    df['nz'] = np.cos(df.zenith) \n",
        "    return df\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def delta_angle(n1, n2, eps=1e-8):\n",
        "    \"\"\" \n",
        "    Calculate angles between two vectors: n1,n2: (B,3) return: (B,) \n",
        "    \"\"\"\n",
        "    n1 = n1 / (np.linalg.norm(n1, axis=1, keepdims=True) + eps)\n",
        "    n2 = n2 / (np.linalg.norm(n2, axis=1, keepdims=True) + eps)\n",
        "    cos = (n1*n2).sum(axis=1).clip(-1,1)\n",
        "    return np.arccos( cos )\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def get_event_features(df, target_df, suf = \"\", aux=True):    \n",
        "    \"\"\" \n",
        "    Aggregated features characterizing the entire event \n",
        "    \"\"\"\n",
        "\n",
        "    df['xt'] = df.x*df.t;  df['yt'] = df.y*df.t; df['zt'] = df.z*df.t;  df['tt'] = df.t**2;       \n",
        "    if aux:\n",
        "        for col in df.columns:\n",
        "            if col not in ['event_id', 'sensor_id', 'line_id']:\n",
        "                df[col] = df[col] * (1-df.aux)\n",
        "\n",
        "    df = df.groupby('event_id').agg(          # this is for all pulses with any aux\n",
        "        tot      = ('t',        'count'),            \n",
        "        t_med    = ('t',        'median'),  \n",
        "        t        = ('t',        'mean'),  \n",
        "        x        = ('x',        'mean'),  \n",
        "        y        = ('y',        'mean'),  \n",
        "        z        = ('z',        'mean'),  \n",
        "        stdT     = ('t',        'std'),\n",
        "        stdX     = ('x',        'std'),\n",
        "        stdY     = ('y',        'std'),\n",
        "        stdZ     = ('z',        'std'),\n",
        "        xt       = ('xt',       'mean'),\n",
        "        yt       = ('yt',       'mean'),\n",
        "        zt       = ('zt',       'mean'),\n",
        "        tt       = ('tt',       'mean'),\n",
        "        q        = ('q',        'mean' ),\n",
        "        q_min    = ('q',        'min' ),\n",
        "        q_max    = ('q',        'max' ),\n",
        "        q_med    = ('q',        'median' ),\n",
        "        aux      = ('aux',      'mean' ),       \n",
        "        core     = ('core',     'mean' ),        \n",
        "        lines    = ('line_id',  'nunique' ),\n",
        "        doms     = ('sensor_id','nunique' ),                \n",
        "    )\n",
        "    df.reset_index(inplace=True)    \n",
        "    \n",
        "    df.aux   = df.aux  .astype(np.float32)\n",
        "    df.lines = df.lines.astype(np.float32)\n",
        "    df.doms  = df.doms .astype(np.float32)    \n",
        "    df.stdT  = df.stdT .astype(np.float32)    \n",
        "    df.stdX  = df.stdX .astype(np.float32)    \n",
        "    df.stdY  = df.stdY .astype(np.float32)    \n",
        "    df.stdZ  = df.stdZ .astype(np.float32)    \n",
        "\n",
        "    df['p_lines'] = np.log10(df.tot / df.lines).astype(np.float32)\n",
        "    df['p_doms']  = np.log10(df.tot / df.doms ).astype(np.float32)    \n",
        "\n",
        "    df.q         = np.log(1+df.q)\n",
        "    df.q_med     = np.log(1+df.q_med)\n",
        "    df.q_min     = np.log(1+df.q_min)\n",
        "    df.q_max     = np.log(1+df.q_max)\n",
        "    df.lines     = np.log10(df.lines)  / 10\n",
        "    df.doms      = np.log10(df.doms)   / 10\n",
        "    df['pulses'] =(np.log10(df.tot)    / 10).astype(np.float32)\n",
        "    \n",
        "    df = df.fillna(0.0)   # if exclude aux is possible problems for std?\n",
        "\n",
        "    if len(suf):          # add a suffix to the column name\n",
        "        cols = [ col + suf for col in df.columns]\n",
        "        df.columns = cols\n",
        "\n",
        "    return df\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def get_pulse_features(df):\n",
        "    \"\"\" \"\"\"\n",
        "    df.drop(columns=['line_id'], inplace=True)   # !!!! (embedding ?)\n",
        "\n",
        "    df.q    = np.log(1+df.q)    \n",
        "    for col in df.columns:\n",
        "        if col not in ['sensor_id', 'event_id', 'line_id', 'tot']:\n",
        "            df[col] = df[col].astype(np.float32)\n",
        "\n",
        "    return df    \n",
        "\n",
        "#===============================================================================\n",
        "#                     Create dataset for train and validation\n",
        "#===============================================================================\n",
        "\n",
        "def get_files(batch_ids):\n",
        "    files = [PATH / f\"batch_{batch_id}.parquet\"  for batch_id in batch_ids]\n",
        "    return files, batch_ids\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def append_dict(data, T, df, agg_df):\n",
        "    \"\"\" \n",
        "    Add dataframes df and agg_df with the given number of pulses T to the data dictionary.\n",
        "    The keys in this dictionary are the number of pulses T.\n",
        "    df:     event_id\tsensor_id\taux\tq\tt  tot\n",
        "    agg_df: event_id, nx, ny, nz, tot, t_aver, ...., ux, uy, uz, qx, qy, qz\n",
        "    \"\"\"\n",
        "    assert len(df) % T == 0,  f\"wait len(df) = T*B, got len={len(df)}, T={T}\"    \n",
        "    B, F = len(df) // T, df.shape[-1] - 3 # drop: event_id, sensor_id, tot\n",
        "    ID   = agg_df[['event_id']].to_numpy()\n",
        "    Y    = agg_df[['nx','ny','nz']].to_numpy()\n",
        "    AGG  = agg_df.iloc[:, 5:].to_numpy()\n",
        "    SENS = df.sensor_id.to_numpy().reshape(B,T)\n",
        "    # (B*T, F) -> (B, T, F) -> (B, F, T) -> (B, F*T)\n",
        "    FEAT= df.iloc[:, 2: -1].to_numpy().reshape(B,T,F)      # drop tot !\n",
        "\n",
        "    assert len(ID)==len(Y) and len(ID)==len(AGG) and len(ID)==len(SENS) and len(ID)==len(FEAT), \\\n",
        "           f\"{ID.shape}, {Y.shape}, {AGG.shape}, {SENS.shape} {FEAT.shape} from df={df.shape} agg_df={agg_df.shape} (T={T},F={F})\"\n",
        "\n",
        "    if T in data:    # ID, Y, AGG, SENS, FEAT \n",
        "        v = data[T]\n",
        "        v[0] = torch.vstack((v[0], torch.tensor(ID,   dtype=torch.long)    ))\n",
        "        v[1] = torch.vstack((v[1], torch.tensor(SENS, dtype=torch.long)    ))\n",
        "        v[2] = torch.cat   ((v[2], torch.tensor(FEAT, dtype=torch.float32) ), dim=0 )\n",
        "        v[3] = torch.vstack((v[3], torch.tensor(AGG,  dtype=torch.float32) ))\n",
        "        v[4] = torch.vstack((v[4], torch.tensor(Y,    dtype=torch.float32) ))\n",
        "        \n",
        "    else:       \n",
        "        data[T] = [torch.tensor(ID,   dtype=torch.long   ),\n",
        "                   torch.tensor(SENS, dtype=torch.long   ),\n",
        "                   torch.tensor(FEAT, dtype=torch.float32),\n",
        "                   torch.tensor(AGG,  dtype=torch.float32),\n",
        "                   torch.tensor(Y,    dtype=torch.float32) ]                  \n",
        "                       \n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def create_dataset(batch_ids, sensors_df, verbose):\n",
        "    \"\"\" \n",
        "    Starting dataset generation\n",
        "    \"\"\"\n",
        "    files, batch_ids = get_files(batch_ids)\n",
        "    data, events_df  = {}, pd.DataFrame({'event_id': []})\n",
        "    for i, (batch_id, fname) in tqdm(enumerate(zip(batch_ids, files))):         \n",
        "        info(f\"******  batch_id: {batch_id:3d}\")\n",
        "        df = pd.read_parquet(fname)            \n",
        "\n",
        "        df = prepare_batch(df)\n",
        "        df = cut_pulses(df, max_pulses=T_MAX)        \n",
        "\n",
        "        df = df.merge(sensors_df, left_on=\"sensor_id\", right_on=\"sensor_id\", how=\"left\")\n",
        "        df = df[['event_id', 'line_id', 'sensor_id', 'core', 'aux', 'q', 't', 'x', 'y', 'z']]\n",
        "        info(f\"merged batch with sensors {df.shape}\")    \n",
        "\n",
        "        target_df = get_target_angles(batch_id=batch_id)\n",
        "        target_df = angles2vector(target_df).drop(columns=['azimuth','zenith'])\n",
        "        info(\"loaded target angles\")\n",
        "\n",
        "        if LINES_FILTER:\n",
        "            df, agg = lines_filter(df, lines_min = LINES_MIN, lines_max = LINES_MAX)  \n",
        "            target_df = target_df[target_df.event_id.isin(agg.event_id)]    \n",
        "            del agg        \n",
        "            info(f\"lines filter done: {df.shape}\")    \n",
        "            #if verbose and i == 0: display(df)\n",
        "\n",
        "        if IS_AGG:\n",
        "            agg_df = get_event_features(df, target_df, suf=\"\", aux=False)\n",
        "            if not DROP_AUX:            \n",
        "                if DOMS_AGG:  # при агригации некоторые сенсоры имеют нецелый aux (умножаем на 1-него)!\n",
        "                    agg2_df = get_event_features(df, target_df, suf=\"_aux\", aux=True)            \n",
        "                else:         \n",
        "                    agg2_df = get_event_features(df[ ~df.aux ].copy(), target_df, suf=\"_aux\", aux=False)            \n",
        "                agg_df = agg_df.merge(agg2_df, left_on='event_id',  right_on='event_id_aux', how='left')\n",
        "                agg_df = agg_df.drop(columns = ['event_id_aux','tot_aux'] )\n",
        "\n",
        "            agg_df = target_df.merge(agg_df, left_on=\"event_id\", right_on=\"event_id\", how=\"left\")        \n",
        "            info('get_event_features done')\n",
        "        else:\n",
        "            agg_df = target_df\n",
        "\n",
        "        df = get_pulse_features(df)                \n",
        "        df = df[['event_id', 'sensor_id', 'aux', 'q', 't']]  #   'core', 'x', 'y', 'z'\n",
        "\n",
        "        info('get_pulse_features done')\n",
        "\n",
        "        if IS_AGG:\n",
        "            df = df.merge(agg_df[['event_id', 'tot']], left_on='event_id', right_on='event_id', how='left')\n",
        "            if verbose and i == 0: show_stats(df, agg_df)\n",
        "\n",
        "        tots = df.tot.unique()\n",
        "        info(f\"count pulses:  {tots.mean():.0f} [{tots.min()} ... {tots.max()}]\")                                    \n",
        "        for n in tqdm(tots): \n",
        "            # first pulse will be last (for RNN)\n",
        "            d1 = df    [df.    tot == n].sort_values(['event_id','t'], ascending=[True,False])           \n",
        "            d2 = agg_df[agg_df.tot == n].sort_values(['event_id']) if IS_AGG else None\n",
        "            append_dict(data, n, d1, d2)\n",
        "        cols_df, cols_agg_df = df.columns, agg_df.columns\n",
        "        del df, agg_df\n",
        "    info(\"collected data for dataset\")                \n",
        "        \n",
        "    return data, events_df.reset_index(drop=True), cols_df, cols_agg_df\n",
        "\n",
        "#===============================================================================\n",
        "#                                Diagnostic\n",
        "#===============================================================================\n",
        "\n",
        "def show_stats(df, agg_df):\n",
        "    \"\"\" \n",
        "    Displaying information about dataframes\n",
        "    \"\"\"\n",
        "    pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
        "    display(df.head(5))\n",
        "    display(df.describe(percentiles=[]).transpose())    \n",
        "    display(df.info())\n",
        "    display(agg_df.head(2))\n",
        "    display(agg_df.describe(percentiles=[]).transpose())                                        \n",
        "    display(agg_df.info())\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def plot_metric(err, prefix=\"\", bins = 200):    \n",
        "    \"\"\" \n",
        "    Build a histogram of errors; calculate the statistics and the share w of 'bad examples' \n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(6,4), facecolor ='w') \n",
        "    plt.axes().set_facecolor(\"ivory\"); plt.autoscale(tight=True)\n",
        "    p,_,_ = plt.hist(err, bins=bins, range=(0,np.pi), fc=\"lightblue\", density=True, alpha=0.5)\n",
        "    w = 2*p[len(p)//2: ].sum()*np.pi/bins    \n",
        "    x = np.linspace(0,np.pi,bins)\n",
        "    plt.plot(x, w * 0.5*np.sin(x),   c=\"darkred\")\n",
        "    plt.plot(x, p-w * 0.5*np.sin(x), c=\"darkblue\")\n",
        "    plt.title(f\"{prefix}mean={np.mean(err):.3f}, median={np.median(err):.3f}, w={w:.3f}\")    \n",
        "    plt.ylabel(\"Density\"); plt.xlabel(r\"$\\Delta \\Psi$ (rad)\"); plt.grid()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlpSy1BBUZge"
      },
      "source": [
        "## Prepare Data and save to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AlZ4FltnlUK"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "info.beg = info.last = time.time()\n",
        "info(\"begin\")\n",
        "\n",
        "if CREATE_DATASET:\n",
        "    sensors_df = get_sensors()    \n",
        "    info(f\"loaded sensors pos: tot={len(sensors_df)}\")\n",
        "    display(sensors_df.head(3))\n",
        "    doms = torch.tensor(sensors_df[['x','y','z','core','a','r']].astype(np.float32).to_numpy())\n",
        "    torch.save( { 'cols': ['x','y','z','core','a','r'], 'data': doms }, f\"{DATASET_FOLDER}/doms.pt\")\n",
        "    \n",
        "    for i,batch_id in tqdm(enumerate(range(FIRST_BATCH_ID, FIRST_BATCH_ID+NUM_BATCHES,  BATCHES_IN_PACK)), total=NUM_BATCHES//BATCHES_IN_PACK):\n",
        "        pack_id = batch_id // BATCHES_IN_PACK + 1\n",
        "        data, _, cols_df, cols_agg_df = create_dataset(range(batch_id, batch_id + BATCHES_IN_PACK), sensors_df, i==0)\n",
        "        torch.save({'cols_df':      cols_df, \n",
        "                    'cols_agg_df':  cols_agg_df, \n",
        "                    'data': data },   f\"{DATASET_FOLDER}/pack_{pack_id:02d}.pt\")        \n",
        "        del data; gc.collect()\n",
        "        info(f\"created pack {pack_id:2d}\")   "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}