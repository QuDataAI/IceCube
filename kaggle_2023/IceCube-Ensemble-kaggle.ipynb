{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Transformer and GNN\n",
        "\n",
        "This notebook builds an ensemble of several variants of the Transformer and GNN model.<br>\n",
        "A more detailed description can be found in the document:<br>\n",
        "https://qudata.com/projects/icecube-neutrino/en/ensemble.html\n",
        "\n",
        "The data for training the ensemble was received in a laptop on kaggle and loaded from google drive"
      ],
      "metadata": {
        "id": "0Ri8qr0NmBWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libs and some functions"
      ],
      "metadata": {
        "id": "BejB-GILpLfi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ri2A8QWToGM"
      },
      "outputs": [],
      "source": [
        "!pip install qunet==0.0.4    # our DL lib\n",
        "!pip install torchinfo\n",
        "\n",
        "import os, gc, sys, time, datetime, math, random, copy, psutil\n",
        "import numpy as np,  matplotlib.pyplot as plt, pandas as pd\n",
        "from   pathlib   import Path        \n",
        "from   tqdm.auto import tqdm\n",
        "import pyarrow, pyarrow.parquet as pq     # read by chanks\n",
        "import torch, torch.nn as nn\n",
        "from   torchinfo import summary\n",
        "\n",
        "from qunet.models   import MLP\n",
        "from qunet.data     import Data\n",
        "from qunet.trainer  import Trainer\n",
        "from qunet.optim    import CosScheduler, ExpScheduler\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def angles2vector(df):\n",
        "    \"\"\" \n",
        "    Add unit vector components from (azimuth,zenith) to the DataFrame df \n",
        "    \"\"\"\n",
        "    df['nx'] = np.sin(df.zenith) * np.cos(df.azimuth)\n",
        "    df['ny'] = np.sin(df.zenith) * np.sin(df.azimuth)\n",
        "    df['nz'] = np.cos(df.zenith) \n",
        "    return df\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def vector2angles(n, eps=1e-8):\n",
        "    \"\"\"  \n",
        "    Get spherical angles of vector n: (B,3) \n",
        "    \"\"\"                \n",
        "    n = n / (np.linalg.norm(n, axis=1, keepdims=True) + eps)    \n",
        "                                \n",
        "    azimuth = np.arctan2( n[:,1],  n[:,0])    \n",
        "    azimuth[azimuth < 0] += 2*np.pi\n",
        "                                \n",
        "    zenith = np.arccos( n[:,2].clip(-1,1) )                                \n",
        "    \n",
        "    return azimuth, zenith\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def angle_errors(n1, n2, eps=1e-8):\n",
        "    \"\"\" \n",
        "    Calculate angles between two vectors:: n1,n2: (B,3) return: (B,) \n",
        "    \"\"\"\n",
        "    n1 = n1 / (np.linalg.norm(n1, axis=1, keepdims=True) + eps)\n",
        "    n2 = n2 / (np.linalg.norm(n2, axis=1, keepdims=True) + eps)\n",
        "    \n",
        "    cos = (n1*n2).sum(axis=1)                     # angles between vectors\n",
        "    angle_err = np.arccos( cos.clip(-1,1) )    \n",
        "        \n",
        "    r1   =  n1[:,0]*n1[:,0] + n1[:,1]*n1[:,1]    # angles between vectors in (x,y)    \n",
        "    r2   =  n2[:,0]*n2[:,0] + n2[:,1]*n2[:,1]\n",
        "    cosX = (n1[:,0]*n2[:,0] + n1[:,1]*n2[:,1]) / (np.sqrt(r1*r2) + eps)    \n",
        "    azimuth_err = np.arccos( cosX.clip(-1,1) )\n",
        "                                \n",
        "    zerros = r1 < eps                            # azimuth angle not defined\n",
        "    azimuth_err[zerros] = np.random.random((len(n1[zerros]),))*np.pi\n",
        "    \n",
        "    zenith1  = np.arccos( n1[:,2].clip(-1,1) )\n",
        "    zenith2  = np.arccos( n2[:,2].clip(-1,1) )\n",
        "    zenith_err = zenith2 - zenith1    \n",
        "        \n",
        "    return angle_err, azimuth_err, zenith_err\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def info(text, pref=\"\", end=\"\\n\"):\n",
        "    \"\"\" \n",
        "    Information about the progress of calculations (time and memory) \n",
        "    \"\"\"\n",
        "    gc.collect()\n",
        "    ram, t = psutil.virtual_memory().used / 1024**3,  time.time()    \n",
        "    print(f\"{pref}{(t-info.beg)/60:5.1f}m[{t-info.last:+5.1f}s] {ram:6.3f}Gb > {text}\",end=end)\n",
        "    info.last = time.time(); \n",
        "info.beg = info.last = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregated features\n",
        "\n",
        "In fact, the aggregated features of the event were not included in the ensemble (they did not improve the competition metric)"
      ],
      "metadata": {
        "id": "OHnTlsSd44Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batches = 80\n",
        "\n",
        "USED_AGG = False\n",
        "if USED_AGG:\n",
        "    info('beg')\n",
        "    agg0_df = pd.read_parquet(f\"/content/drive/MyDrive/IceCube-Dataset/AGG/agg0_batch_1_{batches}.parquet\") \n",
        "    display(agg0_df.head(1))\n",
        "    print(agg0_df.shape)\n",
        "    #agg = torch.Tensor(agg0_df.iloc[:, 1:].to_numpy()).to(torch.float32)\n",
        "    info('del')\n",
        "    agg = torch.Tensor(agg0_df[['lines','doms','core','pulses','qx','qy','qz','Ixx','Iyy','Izz','Ixy','Ixz','Iyz']].to_numpy()).to(torch.float32)\n",
        "    #agg = torch.Tensor(agg0_df[['lines','doms','core','pulses']].to_numpy()).to(torch.float32)\n",
        "    del agg0_df; gc.collect()\n",
        "    info(f'end size:{agg.numel()*4/1024**3:.3f}Gb')\n",
        "else:\n",
        "    agg = None\n"
      ],
      "metadata": {
        "id": "MvhFZfwj49Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Current models\n",
        "\n",
        "For each architecture, 3 models were selected, the results of which on the first 80 batches of training data are loaded from a google drive.\n",
        "\n",
        "First, we calculate the error of each model and the ensemble based on the simple mean."
      ],
      "metadata": {
        "id": "HkbM9toLm1Z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info(\"start models\")\n",
        "\n",
        "# Target values of the direction vector component for the first 80 batches:\n",
        "true_df = pd.read_parquet(f\"/content/drive/MyDrive/IceCube/IceCube-Dataset/AGG/true_batch_1_{batches}.parquet\") \n",
        "display(true_df.head(2))\n",
        "\n",
        "models = [        \n",
        "    # GNN:\n",
        "    f\"/content/drive/MyDrive/IceCube/IceCube-Dataset/AGG/Models/submission_gnn_id_26_val_0_9926_exp_classific_16n_e1633_batch_1_{batches}.csv\",  # gnn1    \n",
        "    f\"/content/drive/MyDrive/IceCube/IceCube-Dataset/AGG/Models/submission_gnn_id_28_val_0_9919_exp_classific_24n_e726_batch_1_{batches}.csv\",   # gnn2    \n",
        "    f\"/content/drive/MyDrive/IceCube/IceCube-Dataset/AGG/Models/submission_gnn_id_27_val_0_9961_exp_readout_emb_e196_batch_1_{batches}.csv\",     # gnn3   \n",
        "\n",
        "\n",
        "    # ATT:    \n",
        "    f\"/content/drive/MyDrive/IceCube/IceCube-Dataset/AGG/Models/submission_att_all_0.9984_L12_batch_1_{batches}.csv\",     # att1    ~AGG_DOMS \n",
        "    f\"/content/drive/MyDrive/IceCube/IceCube-Dataset/AGG/Models/submission_att_rnn_1.0015_L10_batch_1_{batches}.csv\",     # att2     AGG_DOMS    \n",
        "    f\"/content/drive/MyDrive/IceCube/IceCube-Dataset/AGG/Models/submission_att04_1.0003_D00_batch_1_{batches}.csv\",       # att4     AGG_DOMS       \n",
        "]\n",
        "\n",
        "models_df = true_df[['event_id','nx','ny','nz']].copy()\n",
        "models_df.rename(columns={\"nx\": \"nx_true\", \"ny\": \"ny_true\", \"nz\": \"nz_true\"}, inplace=True)\n",
        "\n",
        "for i, m in tqdm(enumerate(models), total=len(models)):\n",
        "    df = pd.read_csv(m)    \n",
        "    print(df.shape, m)\n",
        "    df = angles2vector(df)[['event_id', 'nx','ny','nz']].copy()    \n",
        "    df.rename(columns={\"nx\": f\"nx_{i+1:0d}\", \"ny\": f\"ny_{i+1:0d}\", \"nz\": f\"nz_{i+1:0d}\"}, inplace=True)\n",
        "    models_df = models_df.merge(df, left_on='event_id', right_on='event_id', how='left')\n",
        "del df\n",
        "\n",
        "n_true = models_df[['nx_true','ny_true','nz_true']].to_numpy()\n",
        "\n",
        "print(f\"total samples: {models_df.shape};  number of nan: {models_df.isna().sum().sum()}\")    \n",
        "\n",
        "#--------------------------------------------------------------------------------------------------\n",
        "B, T, E = len(models_df), len(models), 3\n",
        "n_val = int(5*B/batches)\n",
        "\n",
        "# calc ensemble with equal weights\n",
        "n_true = models_df[['nx_true','ny_true','nz_true']].to_numpy()\n",
        "n_pred = np.zeros_like(n_true)\n",
        "for i in range(len(models)):\n",
        "    n = models_df.iloc[:, 4+i*3 : 4+(i+1)*3].to_numpy()        \n",
        "    ang_err, az_err, ze_err = angle_errors(n[:n_val], n_true[:n_val])    \n",
        "    errs = ang_err.reshape(-1,1) if i==0 else np.column_stack([errs, ang_err.reshape(-1,1)])                             # norm: {np.linalg.norm(n, axis=1).mean():.2f} \n",
        "    print(f\"{i:3d}  > ang_err: {ang_err.mean():.4f}; az_err: {az_err.mean():.3f} ze_err: {np.abs(ze_err).mean():.3f}  | {models[i][61:]}\") \n",
        "    n_pred += n\n",
        "\n",
        "print(\"---------------------------------------------------------------\")\n",
        "ang_err, az_err, ze_err = angle_errors(n_pred[:n_val], n_true[:n_val])\n",
        "print(f\"mean > ang_err: {ang_err.mean():.4f}; az_err: {az_err.mean():.3f} ze_err: {np.abs(ze_err).mean():.3f}\")\n",
        "\n",
        "corr = pd.DataFrame(errs).corr()\n",
        "display(corr)\n",
        "#plt.matshow(corr, cmap=\"bwr\"); plt.show()\n"
      ],
      "metadata": {
        "id": "Y32QB9UFT6f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "We form the training and validation dataset.\n",
        "Validation is carried out on the first 5 batches."
      ],
      "metadata": {
        "id": "tFe04dD5jF49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    loss   = 'cos'\n",
        "    ka_reg = 0    \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "QoBMFe9SsdK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info('beg')\n",
        "B, T, E = len(models_df), len(models), 3\n",
        "\n",
        "print(f\"B:{B}, T:{T}, E:{E}\")\n",
        "X = torch.Tensor(models_df.iloc[:, 4:].to_numpy()).view(B,T,E)   # (B,T,E)\n",
        "Y = torch.Tensor(models_df.iloc[:,1:4].to_numpy())               # (B,E)\n",
        "\n",
        "#del models_df  # if there is not enough memory!!!!\n",
        "\n",
        "if agg is None:\n",
        "    agg = Y\n",
        "\n",
        "info('data')\n",
        "n_val = int(5*B/batches)                                         # validate on the first 5 batches\n",
        "data_val = Data( [X[:n_val], agg[:n_val],  Y[:n_val]], batch_size=4*1024,  device=CFG.device, shuffle=False, whole_batch=False)\n",
        "data_trn = Data( [X[n_val:], agg[n_val:],  Y[n_val:]], batch_size=8*1024,  device=CFG.device, shuffle=True,  whole_batch=True,  n_packs=4)\n",
        "del agg; del X; del Y;\n",
        "gc.collect()\n",
        "info('end')"
      ],
      "metadata": {
        "id": "EiQskic6jHtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "0u7yfjEuexLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(x, Y, eps=1e-8):\n",
        "    \"\"\" \n",
        "    loss of the model and angle error (score of competition)\n",
        "    \"\"\"\n",
        "    kappa = torch.norm(x, dim=1, keepdim=True).clip(eps)\n",
        "    y = x / kappa \n",
        "    cos  = (y*Y).sum(dim=1) \n",
        "    if   CFG.loss == 'cos':\n",
        "        loss = 1 - cos.mean() + CFG.ka_reg * (kappa**2).mean()\n",
        "    elif   CFG.loss == 'mse':\n",
        "        loss = (x-Y).pow(2).sum(-1).mean()\n",
        "    elif CFG.loss == 'prod':\n",
        "        loss = -((x*Y).sum(dim=1)).mean() + CFG.ka_reg * kappa.mean()\n",
        "    elif CFG.loss == 'vMF':\n",
        "        logC = -kappa + torch.log( ( kappa+eps )/( 1-torch.exp(-2*kappa)+2*eps ) )\n",
        "        loss = -( (x*Y).sum(dim=1) + logC ).mean() \n",
        "    elif CFG.loss == 'k2':             \n",
        "        loss = -((x*Y).sum(dim=1)).mean() + 0.5 * (kappa**2).mean()\n",
        "    elif CFG.loss == 'k2ze':\n",
        "        loss = -((x*Y).sum(dim=1)).mean() + 0.5 * (kappa**2).mean() + torch.square(y[:,2]-Y[:,2]).mean()\n",
        "\n",
        "    err  = torch.abs( torch.arccos(  torch.clip(cos.detach() ,-1,1) ) )\n",
        "    return loss,  y.detach(),  torch.cat([err.view(-1,1), kappa.detach().view(-1,1)], dim=1 )\n",
        "\n",
        "#-------------------------------------------------------------------------------    \n",
        "\n",
        "class MLP_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    All model outputs, possibly supplemented by aggregated event features, are fed to the MLP input\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: dict):        \n",
        "        super().__init__() \n",
        "        self.cfg = {\n",
        "            'name':      'mlp',\n",
        "            'n_models':  2,\n",
        "            'is_agg':    False,        \n",
        "            'AF':        31,                        # number of aggregated features\n",
        "            'AC':        2,                         # number of compressed aggregated features\n",
        "            'hidden':    256,\n",
        "            'drop':      0.01,\n",
        "        }   \n",
        "        if type(cfg) is dict:                       # add, change properties\n",
        "            self.cfg.update(copy.deepcopy(cfg))\n",
        "        cfg = self.cfg\n",
        "        \n",
        "        self.mlpf = MLP( dict(input=cfg['AF'], scale=4, output=cfg['AC'], drop=cfg['drop']) )  \\\n",
        "                    if  cfg['is_agg'] else None\n",
        "\n",
        "        F = cfg['n_models']*3\n",
        "        if cfg['is_agg']:  F += cfg['AC']\n",
        "        self.mlp = MLP( dict(input=F, hidden=cfg['hidden'], output=3, drop=cfg['drop']) )        \n",
        "\n",
        "    def forward(self, batch, eps=1e-8):      \n",
        "        x,  agg, Y = batch\n",
        "        B,T,E = x.shape\n",
        "        x = x.view(B, T*E)     \n",
        "\n",
        "        if self.mlpf is not None:\n",
        "            agg = self.mlpf(agg)\n",
        "            agg = nn.Tanh()(agg)\n",
        "            x = torch.cat([x, agg], dim=-1)\n",
        "    \n",
        "        x = self.mlp(x)\n",
        "\n",
        "        return metrics(x,Y)"
      ],
      "metadata": {
        "id": "gtNz8LkKfINb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Model"
      ],
      "metadata": {
        "id": "FsfcSHOybDgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP_Model ( dict(n_models=len(models), hidden=1024*2, is_agg=False, AF=13, AC=3, drop=0.2 ) )    \n",
        "\n",
        "model.to(CFG.device)\n",
        "display(summary(model, col_width = 20))\n",
        "print(model.cfg)\n",
        "\n",
        "trainer = Trainer(model, data_trn, data_val)\n",
        "trainer.scheduler = CosScheduler()\n",
        "\n",
        "trainer.set_optimizer( torch.optim.Adam(model.parameters(), lr=1e-3) ) # weight_decay=1e-4\n",
        "trainer.copy_best_model = True"
      ],
      "metadata": {
        "id": "cgkRMSkUbY3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "Q0eeMzFzzTqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(\"batches:\", batches)\n",
        "CFG.loss = 'cos'\n",
        "trainer.scheduler.set(lr1=1e-4, lr_hot=8e-3, lr2=2e-3, samples=200e6, warmup=20e6)\n",
        "trainer.cfg['plot_score'].update(dict(y_min=0.970, y_max=0.990, ticks=21))\n",
        "trainer.run(epochs=1000, \n",
        "            stop_after_samples = 200e6, \n",
        "            samples_beg = 100e6,    # потом включается period_val=1\n",
        "            period_val_beg = 4,     # период валидации на первых samples_beg примерах            \n",
        "            period_plot=20)\n",
        "print()\n"
      ],
      "metadata": {
        "id": "M5Q_yZIkplBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trainer.scheduler = ExpScheduler( trainer.optim)\n",
        "trainer.set_optimizer( torch.optim.Adam(model.parameters(), lr=1e-3) )\n",
        "trainer.scheduler.set(lr1=2e-3,  lr2=5e-4, samples=200e6)\n",
        "trainer.run(epochs=1000, \n",
        "            stop_after_samples = 200e6, \n",
        "            period_plot=20)"
      ],
      "metadata": {
        "id": "gil0_ffI_un9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving, checking"
      ],
      "metadata": {
        "id": "p3LV9_f3sA_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    folder = \"/content/drive/MyDrive/IceCube-Ensemble/\"\n",
        "    fname  = f\"ens_err_{trainer.best_score_val:.4f}_mlp2048_aggF_gnn_1_2_3_att_1_2_4.pt\"\n",
        "    trainer.labels = []\n",
        "    trainer.sample = trainer.samples\n",
        "    trainer.save(folder+fname, trainer.model_best_score, info=\"MLP\")"
      ],
      "metadata": {
        "id": "a8j0rM90DCqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    losses, scores, counts, (_,_,tm_trn) = trainer.fit(0,trainer.model_best_score, data_val, train=False)\n",
        "    loss_val, score_val = trainer.mean(losses, scores, counts)\n",
        "    print(score_val)"
      ],
      "metadata": {
        "id": "WxFOUmQ19WoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble use"
      ],
      "metadata": {
        "id": "5zPBhixDr9fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ensemble_model(fname):\n",
        "    state = torch.load(fname)  \n",
        "    print(state['config'])        \n",
        "    model = MLP_Model(state['config'])    \n",
        "    model.load_state_dict(state['model'])     \n",
        "    return model, state['data']\n",
        "\n",
        "if True:\n",
        "    # Загружаем:\n",
        "    fname = '/content/drive/MyDrive/IceCube/IceCube-Ensemble/ens_err_0.9792_mlp2048_aggF_gnn_1_2_3_att_1_2_4.pt'\n",
        "    model, data = load_ensemble_model(fname)\n",
        "    model.to(CFG.device)\n",
        "\n",
        "    print(model)\n",
        "\n",
        "    trainer = Trainer(model, None, None)\n",
        "    trainer.plotter.plot(trainer.cfg, model, data, w=12, h=4)\n",
        "\n",
        "    # Создаём датасет:\n",
        "    B, T, E = len(models_df), len(models), 3\n",
        "    X = torch.Tensor(models_df.iloc[:, 4:].to_numpy()).view(B,T,E)   # (B,T,E)\n",
        "    Y = torch.Tensor(models_df.iloc[:,1:4].to_numpy())               # (B,E)\n",
        "    data_tst = Data( (X, Y,  Y), batch_size=1024,  device=CFG.device, shuffle=False, whole_batch=False)\n",
        "\n",
        "    output, losses, score = trainer.predict(model, data_tst, verbose=True)\n",
        "    print()\n",
        "    print(f\"output: {output.shape}  err:{score.mean(0)[0]:.4f}   kappa:{score.mean(0)[1]:.4f}\")"
      ],
      "metadata": {
        "id": "UkA8CB1poB6z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}